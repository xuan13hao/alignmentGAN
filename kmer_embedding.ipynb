{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "import sklearn.preprocessing\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWord_model(word,num_features,min_count, model, Unfile):\n",
    "\tword_model = \"\"\n",
    "\tif not os.path.isfile(model):\n",
    "\t\tsentence = LineSentence(Unfile,max_sentence_length = 15000)\n",
    "\n",
    "\t\tnum_features = int(num_features)\n",
    "\t\tmin_word_count = int(min_count)\n",
    "\t\tnum_workers = 20\n",
    "\t\tcontext = 20\n",
    "\t\tdownsampling = 1e-3\n",
    "\n",
    "\t\tprint (\"Training Word2Vec model...\")\n",
    "\t\tword_model = Word2Vec(sentence, workers=num_workers,\\\n",
    "\t\t\t\t\t\tvector_size=num_features, min_count=min_word_count, \\\n",
    "\t\t\t\t\t\twindow=context, sample=downsampling, seed=1)\n",
    "\t\tword_model.init_sims(replace=False)\n",
    "\t\tword_model.save(model)\n",
    "\n",
    "\telse:\n",
    "\t\tprint (\"Loading Word2Vec model...\")\n",
    "\t\tword_model = Word2Vec.load(model)\n",
    "\n",
    "\treturn word_model\n",
    "\n",
    "def DNAToWord(dna, K):\n",
    "\n",
    "\tsentence = \"\"\n",
    "\tlength = len(dna)\n",
    "\n",
    "\tfor i in range(length - K + 1):\n",
    "\t\tsentence += dna[i: i + K] + \" \"\n",
    "\n",
    "\tsentence = sentence[0 : len(sentence) - 1]\n",
    "\treturn sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDNA_split(DNAdata,word):\n",
    "\tlist1 = []\n",
    "\tfor DNA in DNAdata:\n",
    "\t\tDNA = str(DNA).upper()\n",
    "\t\tlist1.append(DNAToWord(DNA,word).split(\" \"))\n",
    "\treturn list1\n",
    "\n",
    "def getAvgFeatureVecs(DNAdata1,model,num_features):\n",
    "\tcounter = 0\n",
    "\tDNAFeatureVecs = np.zeros((len(DNAdata1),2*num_features), dtype=\"float32\")\n",
    "\t\n",
    "\tfor DNA in DNAdata1:\n",
    "\t\tif counter % 1000 == 0:\n",
    "\t\t\tprint (\"DNA %d of %d\\r\" % (counter, len(DNAdata1)))\n",
    "\t\t\tsys.stdout.flush()\n",
    "\t\t# print(DNA)\n",
    "\t\tDNAFeatureVecs[counter][0:num_features] = np.mean(model.wv[DNA],axis = 0)\n",
    "\t\tcounter += 1\n",
    "\t# print()\n",
    "\treturn DNAFeatureVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npyTosvm(npyfile, svmfile, pos_num):\n",
    "\tdataDataVecs = np.load(npyfile)\n",
    "\tg = open(svmfile,'w')\n",
    "\tprint(len(dataDataVecs))\n",
    "\t#print(dataDataVecs[0])\n",
    "\tm = 0\n",
    "\tfor i in range(len(dataDataVecs)):\n",
    "\t\tline = ''\n",
    "\t\tfor j in range(len(dataDataVecs[0])):\n",
    "\t\t\tif j == len(dataDataVecs[0])-1:\n",
    "\t\t\t\tline += str(j+1)+':'+str(dataDataVecs[i][j])+'\\n'\n",
    "\t\t\telse:\n",
    "\t\t\t\tline += str(j+1)+':'+str(dataDataVecs[i][j])+'\\t'\n",
    "\t\tm += 1\n",
    "\t\tif m < (pos_num+1):\n",
    "\t\t\tg.write('1\\t'+line)\n",
    "\t\telse:\n",
    "\t\t\tg.write('0\\t'+line)\n",
    "\n",
    "def SVMtoCSV(svmfile, csvfile):\n",
    "\tf = open(svmfile,'r')\n",
    "\tg = open(csvfile,'w')\n",
    "\tlines = f.readlines()\n",
    "\tlegth = len(lines[0].split('\t'))-1\n",
    "\t#print(legth)\n",
    "\tclassline = 'class'\n",
    "\tfor i in range(legth):\n",
    "\t\tclassline += ',%d'%(i+1)\n",
    "\tg.write(classline+'\\n')\n",
    "\n",
    "\tfor line in lines:\n",
    "\t\tline = line.strip('\\n').split('\t')\n",
    "\t\tg.write(line[0]+',')\n",
    "\n",
    "\t\tlegth2 = len(line[1:])\n",
    "\t\tm = 0\n",
    "\t\tfor j in line[1:]:\n",
    "\t\t\tif m == legth2-1:\n",
    "\t\t\t\tj = j.split(':')[-1]\n",
    "\t\t\t\tg.write(j)\n",
    "\t\t\t\tm += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tj = j.split(':')[-1]\n",
    "\t\t\t\tg.write(j+',')\n",
    "\t\t\t\tm += 1\n",
    "\t\tg.write('\\n')\n",
    "\n",
    "\tf.close()\n",
    "\tg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "DNA 0 of 1\n",
      "[[-3.56030874e-02  2.21278146e-01 -7.31907636e-02  1.48311526e-01\n",
      "   4.82641794e-02 -3.07605505e-01  3.29198912e-02  4.26286548e-01\n",
      "  -2.76535690e-01 -3.44372302e-01 -2.53789071e-02 -1.74097657e-01\n",
      "   4.79766652e-02  1.20833375e-01  2.49048799e-01 -1.55842423e-01\n",
      "   1.61440238e-01 -9.54385549e-02 -1.73393264e-01 -4.63919997e-01\n",
      "   9.30537283e-02  5.26456796e-02  2.52963752e-01 -1.22236460e-01\n",
      "  -1.20321281e-01 -4.14191708e-02 -4.44701314e-01  8.91519487e-02\n",
      "  -1.40201047e-01 -1.13415811e-02  2.87023932e-01 -1.90980345e-01\n",
      "   1.54966846e-01 -3.45687211e-01 -3.63002345e-02  2.11293876e-01\n",
      "  -2.21147109e-03  3.39878015e-02 -1.72865063e-01 -2.46055990e-01\n",
      "   3.51682082e-02 -1.28891855e-01 -4.06381227e-02  4.11482826e-02\n",
      "   1.16385400e-01 -2.85764426e-01 -2.11745679e-01 -2.98926234e-01\n",
      "   5.67863807e-02  1.12395242e-01  1.01764299e-01 -2.93301731e-01\n",
      "  -1.47268534e-01 -6.63441122e-02  1.12867383e-02 -1.01097286e-01\n",
      "   2.21956372e-01  1.72834918e-02 -1.09176375e-01 -3.19948653e-04\n",
      "   1.19651429e-01 -6.56595267e-03  3.31792653e-01  1.05297901e-01\n",
      "  -2.61514544e-01  2.47895509e-01 -1.26858698e-02  1.91791341e-01\n",
      "  -4.63449121e-01  9.57811996e-02 -1.16592884e-01  2.36263707e-01\n",
      "   2.18819052e-01  9.85800475e-02  4.03287172e-01 -4.25251871e-02\n",
      "  -9.16018859e-02  1.32307038e-01 -3.12114954e-01 -3.80095094e-02\n",
      "  -2.35978380e-01  8.47328976e-02 -3.34446877e-01  4.70183820e-01\n",
      "  -3.24977905e-01 -2.47300491e-02  1.49066940e-01  4.19621132e-02\n",
      "   4.51626070e-02  1.41141057e-01  2.23854125e-01  1.23610504e-01\n",
      "  -2.44835000e-02  7.28041306e-03  3.68443519e-01  4.39762585e-02\n",
      "  -6.02323376e-02  2.07159333e-02 -9.99183282e-02 -6.82990998e-02\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "kmer = 3\n",
    "pos_number = 10 # NOTE: the number of postive sample in test file\n",
    "seq_reads = []\n",
    "import fasta\n",
    "for record in fasta.parse('test.fa'):\n",
    "    seq_reads.append(str(record.seq))\n",
    "#### generate Unsupervised ##### \n",
    "Unfile = '%dUn'%(kmer)\n",
    "g = open(Unfile,'w')\n",
    "words1 = getDNA_split(seq_reads,kmer)\n",
    "\n",
    "for i in range(len(words1)):\n",
    "\tline = ' '.join(words1[i])\n",
    "\tg.write(line+'\\n')\n",
    "g.close()\n",
    "\n",
    "model = 'model_%d'%(kmer)\n",
    "fea_num = 100\n",
    "min_fea = 10\n",
    "wm = getWord_model(kmer,fea_num,min_fea,model,Unfile)\n",
    "word_model = Word2Vec.load(model)\n",
    "dataDataVecs = getAvgFeatureVecs(words1,word_model,fea_num)\n",
    "print (dataDataVecs)\n",
    "fea_npy = '%d_vecs.npy'%(kmer)\n",
    "np.save(fea_npy,dataDataVecs)\n",
    "\n",
    "\n",
    "# #### npy To csv #####\n",
    "# fea_svm = '%d_vecs.svm'%(kmer)\n",
    "# fea_csv = '%d_vecs.csv'%(kmer)\n",
    "\n",
    "# npyTosvm(fea_npy, fea_svm,pos_number)\n",
    "# SVMtoCSV(fea_svm, fea_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
