{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from data_iter import DisDataIter, GenDataIter\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "import generator_kmer\n",
    "import discriminator_kmer\n",
    "import target_lstm_kmer\n",
    "from generator_kmer import Generator\n",
    "from discriminator_kmer import Discriminator\n",
    "from target_lstm_kmer import TargetLSTM\n",
    "from loss import PGLoss\n",
    "\n",
    "import preprocessdna\n",
    "import load_data\n",
    "import loss\n",
    "import os\n",
    "\n",
    "# Arguemnts\n",
    "parser = argparse.ArgumentParser(description='SeqGAN')\n",
    "parser.add_argument('--hpc', action='store_true', default=False,\n",
    "                    help='set to hpc mode')\n",
    "parser.add_argument('--data_path', type=str, default='/scratch/zc807/seq_gan/', metavar='PATH',\n",
    "                    help='data path to save files (default: /scratch/zc807/seq_gan/)')\n",
    "parser.add_argument('--rounds', type=int, default=2, metavar='N',\n",
    "                    help='rounds of adversarial training (default: 150)')\n",
    "parser.add_argument('--g_pretrain_steps', type=int, default=2, metavar='N',\n",
    "                    help='steps of pre-training of generators (default: 120)')\n",
    "parser.add_argument('--d_pretrain_steps', type=int, default=2, metavar='N',\n",
    "                    help='steps of pre-training of discriminators (default: 50)')\n",
    "parser.add_argument('--g_steps', type=int, default=1, metavar='N',\n",
    "                    help='steps of generator updates in one round of adverarial training (default: 1)')\n",
    "parser.add_argument('--d_steps', type=int, default=1, metavar='N',\n",
    "                    help='steps of discriminator updates in one round of adverarial training (default: 3)')\n",
    "parser.add_argument('--gk_epochs', type=int, default=1, metavar='N',\n",
    "                    help='epochs of generator updates in one step of generate update (default: 1)')\n",
    "parser.add_argument('--dk_epochs', type=int, default=1, metavar='N',\n",
    "                    help='epochs of discriminator updates in one step of discriminator update (default: 3)')\n",
    "parser.add_argument('--update_rate', type=float, default=0.8, metavar='UR',\n",
    "                    help='update rate of roll-out model (default: 0.8)')\n",
    "parser.add_argument('--n_rollout', type=int, default=1, metavar='N',\n",
    "                    help='number of roll-out (default: 16)')\n",
    "parser.add_argument('--vocab_size', type=int, default=10, metavar='N',\n",
    "                    help='vocabulary size (default: 10)')\n",
    "parser.add_argument('--batch_size', type=int, default=2, metavar='N',\n",
    "                    help='batch size (default: 64)')\n",
    "parser.add_argument('--n_samples', type=int, default=2, metavar='N',\n",
    "                    help='number of samples gerenated per time (default: 6400)')\n",
    "parser.add_argument('--gen_lr', type=float, default=1e-3, metavar='LR',\n",
    "                    help='learning rate of generator optimizer (default: 1e-3)')\n",
    "parser.add_argument('--dis_lr', type=float, default=1e-3, metavar='LR',\n",
    "                    help='learning rate of discriminator optimizer (default: 1e-3)')\n",
    "parser.add_argument('--no_cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "\n",
    "# Discriminator Parameters\n",
    "Tensor =  torch.LongTensor\n",
    "d_num_class = 2\n",
    "d_embed_dim = 64\n",
    "d_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "d_dropout_prob = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading DNA to vec model ...\n",
      "get words #64\n",
      "Total words: 66\n",
      "torch.Size([13, 148])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m         z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(real_embedding\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],real_embedding\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m     42\u001b[0m         \u001b[39m# print(z.dtype)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m         gen_word \u001b[39m=\u001b[39m generator(z)\n\u001b[1;32m     44\u001b[0m \u001b[39m#         print(gen_word)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[39m# discriminator(gen_word)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         \u001b[39m# gen_optimizer.zero_grad()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[39m# d_loss.backward()\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         \u001b[39m# dis_optimizer.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/alignmentGAN/generator_kmer.py:32\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm\u001b[39m.\u001b[39mflatten_parameters()\n\u001b[1;32m     31\u001b[0m h0, c0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed(x) \u001b[39m# batch_size * seq_len * emb_dim \u001b[39;00m\n\u001b[1;32m     33\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(emb, (h0, c0)) \u001b[39m# out: batch_size * seq_len * hidden_dim\u001b[39;00m\n\u001b[1;32m     34\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_softmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim))) \u001b[39m# (batch_size*seq_len) * vocab_size\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "path_prefix = './'\n",
    "# args = parser.parse_args()\n",
    "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "train_seq = load_data.get_seqs(\"train.fa\")\n",
    "test_seq = load_data.get_seqs(\"test.fa\")\n",
    "#     # print(seq_list)\n",
    "train_kmer_list = load_data.getKmerList(train_seq,3)\n",
    "test_kmer_list = load_data.getKmerList(test_seq,3)\n",
    "ref_model = load_data.train_word2vec(test_kmer_list)\n",
    "# print(model)\n",
    "#     # print(\"saving model ...\")\n",
    "model = load_data.train_word2vec(train_kmer_list + test_kmer_list)\n",
    "model.save(os.path.join(path_prefix, 'w2v_all.model'))\n",
    "ref_model.save(os.path.join(path_prefix, 'ref.model')) #将模型保存这一步可以使得后续的训练更方便，是一个很好的习惯\n",
    "g_embed_dim = 64\n",
    "g_hidden_dim = 32\n",
    "#     # # 150 - 3 + 1\n",
    "train_preprocess = preprocessdna.Preprocess(train_kmer_list, 148, \"w2v_all.model\")\n",
    "real_embedding = train_preprocess.make_embedding(load=True)\n",
    "real_data = train_preprocess.sentence_word2idx()\n",
    "# real_embedding = Tensor(real_embedding)\n",
    "# train_preprocess = preprocessdna.Preprocess(train_kmer_list, 67, \"ref.model\")\n",
    "# ref_embedding = train_preprocess.make_embedding(load=True)\n",
    "print(real_data.size())\n",
    "#     # # def __init__(self,  pre_weight,vocab_size, embedding_dim, hidden_dim, use_cuda):\n",
    "target_lstm = target_lstm_kmer.TargetLSTM(real_embedding, g_embed_dim, g_hidden_dim, False)\n",
    "# print(target_lstm)\n",
    "generator = Generator(real_embedding, g_embed_dim, g_hidden_dim, False)\n",
    "discriminator = Discriminator(real_embedding,d_num_class, d_embed_dim, d_filter_sizes, d_num_filters, d_dropout_prob)\n",
    "# # print(generator)\n",
    "# # print(discriminator)\n",
    "nll_loss = nn.NLLLoss()\n",
    "pg_loss = PGLoss()\n",
    "\n",
    "gen_optimizer = optim.Adam(params=generator.parameters(), lr=1e-3)\n",
    "dis_optimizer = optim.SGD(params=discriminator.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i, real_word in enumerate(real_embedding):\n",
    "        # z = Tensor(np.random.normal(0, 1, (real_embedding.shape[0], real_embedding.shape[1])))\n",
    "        z = torch.randn(real_embedding.shape[0],real_embedding.shape[1])\n",
    "        # print(z.dtype)\n",
    "        gen_word = generator(z)\n",
    "#         print(gen_word)\n",
    "        # discriminator(gen_word)\n",
    "        # gen_optimizer.zero_grad()\n",
    "        # g_loss = nll_loss(discriminator(gen_word),torch.ones(real_embedding.size(0),1))\n",
    "\n",
    "        # g_loss.backward()\n",
    "        # gen_optimizer.step()\n",
    "        # dis_optimizer.zero_grad()\n",
    "\n",
    "        # real_loss = nll_loss(discriminator(real_word), torch.ones(real_embedding.size(0),1))\n",
    "        # fake_loss = nll_loss(discriminator(gen_word.detach()), gen_word)\n",
    "        # d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # d_loss.backward()\n",
    "        # dis_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
