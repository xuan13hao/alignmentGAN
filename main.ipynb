{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuan/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  87, 4410, 3560,  ..., 4767, 4973,  619],\n",
      "        [4766,  468, 2145,  ..., 2707, 1370, 2515],\n",
      "        [4665, 4080, 2572,  ..., 1166, 4257, 3687],\n",
      "        ...,\n",
      "        [ 897, 2187,  557,  ...,  973, 3541, 4199],\n",
      "        [1877, 4241,  295,  ..., 4311, 1110, 4311],\n",
      "        [1144, 3528, 2032,  ..., 2318, 1271, 3479]])\n",
      "epoch 1 : .."
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import generator\n",
    "import discriminator\n",
    "import generator_kmer\n",
    "import discriminator_kmer\n",
    "\n",
    "import helpers\n",
    "\n",
    "import kmer_embedding\n",
    "CUDA = False\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_SEQ_LEN = 20\n",
    "START_LETTER = 0\n",
    "BATCH_SIZE = 1\n",
    "MLE_TRAIN_EPOCHS = 2\n",
    "ADV_TRAIN_EPOCHS = 2\n",
    "POS_NEG_SAMPLES = 10000\n",
    "\n",
    "GEN_EMBEDDING_DIM = 32\n",
    "GEN_HIDDEN_DIM = 32\n",
    "DIS_EMBEDDING_DIM = 64\n",
    "DIS_HIDDEN_DIM = 64\n",
    "\n",
    "oracle_samples_path = './oracle_samples.trc'\n",
    "oracle_state_dict_path = './oracle_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "pretrained_gen_path = './gen_MLEtrain_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "pretrained_dis_path = './dis_pretrain_EMBDIM_64_HIDDENDIM64_VOCAB5000_MAXSEQLEN20.trc'\n",
    "\n",
    "\n",
    "def train_generator_MLE(gen, gen_opt, oracle, real_data_samples, epochs):\n",
    "    # print(\"oracle = \",oracle)\n",
    "    # print(\"real_data_samples = \",real_data_samples)\n",
    "    \"\"\"\n",
    "    Max Likelihood Pretraining for the generator\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch %d : ' % (epoch + 1), end='')\n",
    "        sys.stdout.flush()\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(0, POS_NEG_SAMPLES, BATCH_SIZE):\n",
    "            inp, target = helpers.prepare_generator_batch(real_data_samples[i:i + BATCH_SIZE], start_letter=START_LETTER,\n",
    "                                                          gpu=CUDA)\n",
    "            gen_opt.zero_grad()\n",
    "            loss = gen.batchNLLLoss(inp, target)\n",
    "            loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "            if (i / BATCH_SIZE) % ceil(\n",
    "                            ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # each loss in a batch is loss per sample\n",
    "        total_loss = total_loss / ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / MAX_SEQ_LEN\n",
    "\n",
    "        # sample from generator and compute oracle NLL\n",
    "        oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
    "\n",
    "        print(' average_train_NLL = %.4f, oracle_sample_NLL = %.4f' % (total_loss, oracle_loss))\n",
    "\n",
    "\n",
    "def train_generator_PG(gen, gen_opt, oracle, dis, num_batches):\n",
    "    \"\"\"\n",
    "    The generator is trained using policy gradients, using the reward from the discriminator.\n",
    "    Training is done for num_batches batches.\n",
    "    \"\"\"\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        s = gen.sample(BATCH_SIZE*2)        # 64 works best\n",
    "        inp, target = helpers.prepare_generator_batch(s, start_letter=START_LETTER, gpu=CUDA)\n",
    "        rewards = dis.batchClassify(target)\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        pg_loss = gen.batchPGLoss(inp, target, rewards)\n",
    "        pg_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "    # sample from generator and compute oracle NLL\n",
    "    oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
    "\n",
    "    print(' oracle_sample_NLL = %.4f' % oracle_loss)\n",
    "\n",
    "\n",
    "def train_discriminator(discriminator, dis_opt, real_data_samples, generator, oracle, d_steps, epochs):\n",
    "    \"\"\"\n",
    "    Training the discriminator on real_data_samples (positive) and generated samples from generator (negative).\n",
    "    Samples are drawn d_steps times, and the discriminator is trained for epochs epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # generating a small validation set before training (using oracle and generator)\n",
    "    pos_val = oracle.sample(100)\n",
    "    neg_val = generator.sample(100)\n",
    "    val_inp, val_target = helpers.prepare_discriminator_data(pos_val, neg_val, gpu=CUDA)\n",
    "\n",
    "    for d_step in range(d_steps):\n",
    "        s = helpers.batchwise_sample(generator, POS_NEG_SAMPLES, BATCH_SIZE)\n",
    "        dis_inp, dis_target = helpers.prepare_discriminator_data(real_data_samples, s, gpu=CUDA)\n",
    "        for epoch in range(epochs):\n",
    "            print('d-step %d epoch %d : ' % (d_step + 1, epoch + 1), end='')\n",
    "            sys.stdout.flush()\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            for i in range(0, 2 * POS_NEG_SAMPLES, BATCH_SIZE):\n",
    "                inp, target = dis_inp[i:i + BATCH_SIZE], dis_target[i:i + BATCH_SIZE]\n",
    "                dis_opt.zero_grad()\n",
    "                out = discriminator.batchClassify(inp)\n",
    "                loss_fn = nn.BCELoss()\n",
    "                loss = loss_fn(out, target)\n",
    "                loss.backward()\n",
    "                dis_opt.step()\n",
    "\n",
    "                total_loss += loss.data.item()\n",
    "                total_acc += torch.sum((out>0.5)==(target>0.5)).data.item()\n",
    "\n",
    "                if (i / BATCH_SIZE) % ceil(ceil(2 * POS_NEG_SAMPLES / float(\n",
    "                        BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
    "                    print('.', end='')\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            total_loss /= ceil(2 * POS_NEG_SAMPLES / float(BATCH_SIZE))\n",
    "            total_acc /= float(2 * POS_NEG_SAMPLES)\n",
    "\n",
    "            val_pred = discriminator.batchClassify(val_inp)\n",
    "            print(' average_loss = %.4f, train_acc = %.4f, val_acc = %.4f' % (\n",
    "                total_loss, total_acc, torch.sum((val_pred>0.5)==(val_target>0.5)).data.item()/200.))\n",
    "\n",
    "# MAIN\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    oracle = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "    oracle.load_state_dict(torch.load(oracle_state_dict_path))\n",
    "    oracle_samples = torch.load(oracle_samples_path).type(torch.LongTensor)\n",
    "    # a new oracle can be generated by passing oracle_init=True in the generator constructor\n",
    "    # samples for the new oracle can be generated using helpers.batchwise_sample()\n",
    "    print(oracle_samples)\n",
    "    gen = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "    dis = discriminator.Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "\n",
    "    # if CUDA:\n",
    "    #     oracle = oracle.cuda()\n",
    "    #     gen = gen.cuda()\n",
    "    #     dis = dis.cuda()\n",
    "    #     oracle_samples = oracle_samples.cuda()\n",
    "\n",
    "    # # GENERATOR MLE TRAINING\n",
    "    # print('Starting Generator MLE Training...')\n",
    "    gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "    train_generator_MLE(gen, gen_optimizer, oracle, oracle_samples, MLE_TRAIN_EPOCHS)\n",
    "\n",
    "    # # torch.save(gen.state_dict(), pretrained_gen_path)\n",
    "    # # gen.load_state_dict(torch.load(pretrained_gen_path))\n",
    "\n",
    "    # # PRETRAIN DISCRIMINATOR\n",
    "    # print('\\nStarting Discriminator Training...')\n",
    "    # dis_optimizer = optim.Adagrad(dis.parameters())\n",
    "    # train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, 2, 2)\n",
    "\n",
    "    # # torch.save(dis.state_dict(), pretrained_dis_path)\n",
    "    # # dis.load_state_dict(torch.load(pretrained_dis_path))\n",
    "\n",
    "    # # ADVERSARIAL TRAINING\n",
    "    # print('\\nStarting Adversarial Training...')\n",
    "    # oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "    #                                            start_letter=START_LETTER, gpu=CUDA)\n",
    "    # print('\\nInitial Oracle Sample Loss : %.4f' % oracle_loss)\n",
    "\n",
    "    # for epoch in range(ADV_TRAIN_EPOCHS):\n",
    "    #     print('\\n--------\\nEPOCH %d\\n--------' % (epoch+1))\n",
    "    #     # TRAIN GENERATOR\n",
    "    #     print('\\nAdversarial Training Generator : ', end='')\n",
    "    #     sys.stdout.flush()\n",
    "    #     train_generator_PG(gen, gen_optimizer, oracle, dis, 1)\n",
    "\n",
    "    #     # TRAIN DISCRIMINATOR\n",
    "    #     print('\\nAdversarial Training Discriminator : ')\n",
    "    #     train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "tensor([[-0.0009,  0.0013,  0.0040,  ..., -0.0059,  0.0008,  0.0069],\n",
      "        [-0.0093,  0.0056,  0.0033,  ..., -0.0004, -0.0099,  0.0054],\n",
      "        [-0.0003,  0.0043, -0.0081,  ...,  0.0019,  0.0080, -0.0062],\n",
      "        ...,\n",
      "        [-0.0063,  0.0052, -0.0095,  ...,  0.0064,  0.0052, -0.0070],\n",
      "        [-0.0030,  0.0007,  0.0015,  ...,  0.0061,  0.0069, -0.0037],\n",
      "        [ 0.0003,  0.0078,  0.0047,  ...,  0.0084, -0.0036, -0.0073]])\n",
      "Loading Word2Vec model...\n",
      "tensor([[-0.0009,  0.0013,  0.0040,  ..., -0.0059,  0.0008,  0.0069],\n",
      "        [-0.0093,  0.0056,  0.0033,  ..., -0.0004, -0.0099,  0.0054],\n",
      "        [-0.0003,  0.0043, -0.0081,  ...,  0.0019,  0.0080, -0.0062],\n",
      "        ...,\n",
      "        [-0.0063,  0.0052, -0.0095,  ...,  0.0064,  0.0052, -0.0070],\n",
      "        [-0.0030,  0.0007,  0.0015,  ...,  0.0061,  0.0069, -0.0037],\n",
      "        [ 0.0003,  0.0078,  0.0047,  ...,  0.0084, -0.0036, -0.0073]])\n",
      "oracle =  tensor([[-0.0009,  0.0013,  0.0040,  ..., -0.0059,  0.0008,  0.0069],\n",
      "        [-0.0093,  0.0056,  0.0033,  ..., -0.0004, -0.0099,  0.0054],\n",
      "        [-0.0003,  0.0043, -0.0081,  ...,  0.0019,  0.0080, -0.0062],\n",
      "        ...,\n",
      "        [-0.0063,  0.0052, -0.0095,  ...,  0.0064,  0.0052, -0.0070],\n",
      "        [-0.0030,  0.0007,  0.0015,  ...,  0.0061,  0.0069, -0.0037],\n",
      "        [ 0.0003,  0.0078,  0.0047,  ...,  0.0084, -0.0036, -0.0073]])\n",
      "real_data_samples =  tensor([[-0.0009,  0.0013,  0.0040,  ..., -0.0059,  0.0008,  0.0069],\n",
      "        [-0.0093,  0.0056,  0.0033,  ..., -0.0004, -0.0099,  0.0054],\n",
      "        [-0.0003,  0.0043, -0.0081,  ...,  0.0019,  0.0080, -0.0062],\n",
      "        ...,\n",
      "        [-0.0063,  0.0052, -0.0095,  ...,  0.0064,  0.0052, -0.0070],\n",
      "        [-0.0030,  0.0007,  0.0015,  ...,  0.0061,  0.0069, -0.0037],\n",
      "        [ 0.0003,  0.0078,  0.0047,  ...,  0.0084, -0.0036, -0.0073]])\n",
      "epoch 1 : .........."
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# oracle_samples = oracle_samples.cuda()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m gen_optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(gen\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m train_generator_MLE(gen, gen_optimizer, kt_embd, kt_embd1, MLE_TRAIN_EPOCHS)\n",
      "Cell \u001b[0;32mIn[16], line 69\u001b[0m, in \u001b[0;36mtrain_generator_MLE\u001b[0;34m(gen, gen_opt, oracle, real_data_samples, epochs)\u001b[0m\n\u001b[1;32m     66\u001b[0m total_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m ceil(POS_NEG_SAMPLES \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(BATCH_SIZE)) \u001b[39m/\u001b[39m MAX_SEQ_LEN\n\u001b[1;32m     68\u001b[0m \u001b[39m# sample from generator and compute oracle NLL\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m oracle_loss \u001b[39m=\u001b[39m helpers\u001b[39m.\u001b[39;49mbatchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n\u001b[1;32m     70\u001b[0m                                            start_letter\u001b[39m=\u001b[39;49mSTART_LETTER, gpu\u001b[39m=\u001b[39;49mCUDA)\n\u001b[1;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m average_train_NLL = \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m, oracle_sample_NLL = \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (total_loss, oracle_loss))\n",
      "File \u001b[0;32m~/alignmentGAN/helpers.py:80\u001b[0m, in \u001b[0;36mbatchwise_oracle_nll\u001b[0;34m(gen, oracle, num_samples, batch_size, max_seq_len, start_letter, gpu)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatchwise_oracle_nll\u001b[39m(gen, oracle, num_samples, batch_size, max_seq_len, start_letter\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, gpu\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 80\u001b[0m     s \u001b[39m=\u001b[39m batchwise_sample(gen, num_samples, batch_size)\n\u001b[1;32m     81\u001b[0m     oracle_nll \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     82\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num_samples, batch_size):\n",
      "File \u001b[0;32m~/alignmentGAN/helpers.py:74\u001b[0m, in \u001b[0;36mbatchwise_sample\u001b[0;34m(gen, num_samples, batch_size)\u001b[0m\n\u001b[1;32m     72\u001b[0m samples \u001b[39m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(ceil(num_samples\u001b[39m/\u001b[39m\u001b[39mfloat\u001b[39m(batch_size)))):\n\u001b[0;32m---> 74\u001b[0m     samples\u001b[39m.\u001b[39mappend(gen\u001b[39m.\u001b[39;49msample(batch_size))\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(samples, \u001b[39m0\u001b[39m)[:num_samples]\n",
      "File \u001b[0;32m~/alignmentGAN/generator_kmer.py:82\u001b[0m, in \u001b[0;36mGenerator.sample\u001b[0;34m(self, num_samples, start_letter)\u001b[0m\n\u001b[1;32m     79\u001b[0m     inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_len):\n\u001b[0;32m---> 82\u001b[0m     out, h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(inp, h)               \u001b[39m# out: num_samples x vocab_size\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmultinomial(torch\u001b[39m.\u001b[39mexp(out), \u001b[39m1\u001b[39m)  \u001b[39m# num_samples x 1 (sampling from each row)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     samples[:, i] \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdata\n",
      "File \u001b[0;32m~/alignmentGAN/generator_kmer.py:57\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, inp, hidden)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mEmbeds input and applies GRU one token at a time (seq_len = 1)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# input dim                                             # batch_size\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(inp)                              \u001b[39m# batch_size x embedding_dim\u001b[39;00m\n\u001b[1;32m     58\u001b[0m emb \u001b[39m=\u001b[39m emb\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_dim)               \u001b[39m# 1 x batch_size x embedding_dim\u001b[39;00m\n\u001b[1;32m     59\u001b[0m out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru(emb, hidden)                     \u001b[39m# 1 x batch_size x hidden_dim (out)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "\n",
    "kt_embd= kmer_embedding.kmer_tensor('test.fa',3,100,3)\n",
    "ori = generator_kmer.Generator(kt_embd, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "print(kt_embd)\n",
    "kt_embd1= kmer_embedding.kmer_tensor('test1.fa',3,100,3)\n",
    "gen = generator_kmer.Generator(kt_embd1, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "dis = discriminator_kmer.Discriminator(kt_embd1, DIS_HIDDEN_DIM, MAX_SEQ_LEN, gpu=CUDA)\n",
    "print(kt_embd1)\n",
    "if CUDA:\n",
    "    oracle = ori.cuda()\n",
    "    gen = gen.cuda()\n",
    "    dis = dis.cuda()\n",
    "    # oracle_samples = oracle_samples.cuda()\n",
    "\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "train_generator_MLE(gen, gen_optimizer, kt_embd, kt_embd1, MLE_TRAIN_EPOCHS)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
